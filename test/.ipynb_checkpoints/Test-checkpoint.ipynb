{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from num2words import num2words\n",
    "import logging\n",
    "import string\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    \"\"\" This function will process the raw text into structural format and tokenize the original text\n",
    "    Args:\n",
    "        text (str): a line of textual sentences that can be split and separate\n",
    "    \n",
    "    Return:\n",
    "        data (:py:class:`pandas.DataFrame`): DataFrame that is ready for feature generation\n",
    "    \"\"\"\n",
    "    if text is None:\n",
    "        raise ValueError(\"The input value is not valid.\")\n",
    "    elif type(text) is not str:\n",
    "        raise TypeError(\"The input type is not string.\")\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    Tokenizer = TweetTokenizer()\n",
    "    ## tokenize the text\n",
    "    tokenized = Tokenizer.tokenize(text)\n",
    "    ## define punctuation\n",
    "    punctuation = list(string.punctuation)\n",
    "    ## remove the last punctuation\n",
    "    punctuation.remove('!')\n",
    "    tokenized_no_punctuation=[word.lower() for word in tokenized if word not in punctuation]\n",
    "    tokenized_no_stopwords=[word for word in tokenized_no_punctuation if word not in stopwords.words('english')]\n",
    "    ## extract the stem\n",
    "    tokens = [PorterStemmer().stem(word) for word in tokenized_no_stopwords if word != '️']\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        try:\n",
    "            tokens[i]=num2words(tokens[i])\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-f746956e9f4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"false\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"123\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text1' is not defined"
     ]
    }
   ],
   "source": [
    "if type(text1) is str:\n",
    "    print(\"false\")\n",
    "elif type(text1) is not str:\n",
    "    print(\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text2 = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    }
   ],
   "source": [
    "if type(text2) is str:\n",
    "    print(\"false\")\n",
    "elif type(text2) is not str:\n",
    "    print(\"123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"happy halahalajohj odjv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happi', 'halahalajohj', 'odjv']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "    test_data = pd.read_csv(\"test_data/test_Tweets.csv\")\n",
    "    result_data = pd.read_csv(\"test_data/test_Tweets_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data[\"tokens_output\"] = test_data[\"text\"].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(result_data[\"tokens\"] == test_data[\"tokens_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_data[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@virginamerica', '@dhepburn', 'said']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process_text(test_data[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    [@virginamerica, @dhepburn, said]\n",
       "1    [@virginamerica, plu, ad, commerci, experi, .....\n",
       "2    [@virginamerica, today, ..., must, mean, need,...\n",
       "3    [@virginamerica, realli, aggress, blast, obnox...\n",
       "4            [@virginamerica, realli, big, bad, thing]\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[\"text\"][:5].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_data = pd.read_csv(\"test_data/test_Tweets_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ['@virginamerica', 'realli', 'aggress', 'blast...\n",
       "1    ['@virginamerica', 'realli', 'big', 'bad', 'th...\n",
       "2    ['@virginamerica', 'serious', 'would', 'pay', ...\n",
       "3    ['@virginamerica', 'amaz', 'arriv', 'hour', 'e...\n",
       "4    ['@virginamerica', '<3', 'pretti', 'graphic', ...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data.iloc[:5][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>tokens</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>['@virginamerica', 'realli', 'aggress', 'blast...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>['@virginamerica', 'realli', 'big', 'bad', 'th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>['@virginamerica', 'serious', 'would', 'pay', ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>['@virginamerica', 'amaz', 'arriv', 'hour', 'e...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>['@virginamerica', '&lt;3', 'pretti', 'graphic', ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8892</td>\n",
       "      <td>American</td>\n",
       "      <td>['@americanair', 'thank', '!']</td>\n",
       "      <td>positive</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8893</td>\n",
       "      <td>American</td>\n",
       "      <td>['@americanair', 'thx', 'noth', 'get', 'us', '...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8894</td>\n",
       "      <td>American</td>\n",
       "      <td>['@americanair', 'flight', 'cancel', 'flightl'...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8895</td>\n",
       "      <td>American</td>\n",
       "      <td>['@americanair', 'leav', 'twenty', 'minut', 'l...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8896</td>\n",
       "      <td>American</td>\n",
       "      <td>['@americanair', 'money', 'chang', 'flight', '...</td>\n",
       "      <td>negative</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8897 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             airline                                             tokens  \\\n",
       "0     Virgin America  ['@virginamerica', 'realli', 'aggress', 'blast...   \n",
       "1     Virgin America  ['@virginamerica', 'realli', 'big', 'bad', 'th...   \n",
       "2     Virgin America  ['@virginamerica', 'serious', 'would', 'pay', ...   \n",
       "3     Virgin America  ['@virginamerica', 'amaz', 'arriv', 'hour', 'e...   \n",
       "4     Virgin America  ['@virginamerica', '<3', 'pretti', 'graphic', ...   \n",
       "...              ...                                                ...   \n",
       "8892        American                     ['@americanair', 'thank', '!']   \n",
       "8893        American  ['@americanair', 'thx', 'noth', 'get', 'us', '...   \n",
       "8894        American  ['@americanair', 'flight', 'cancel', 'flightl'...   \n",
       "8895        American  ['@americanair', 'leav', 'twenty', 'minut', 'l...   \n",
       "8896        American  ['@americanair', 'money', 'chang', 'flight', '...   \n",
       "\n",
       "     airline_sentiment  positive  \n",
       "0             negative         0  \n",
       "1             negative         0  \n",
       "2             negative         0  \n",
       "3             positive         1  \n",
       "4             positive         1  \n",
       "...                ...       ...  \n",
       "8892          positive         1  \n",
       "8893          negative         0  \n",
       "8894          negative         0  \n",
       "8895          negative         0  \n",
       "8896          negative         0  \n",
       "\n",
       "[8897 rows x 4 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"test_data/test_Tweets_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process_text_happy():\n",
    "    \"\"\"\n",
    "    Test the functionality of process text input\n",
    "    \"\"\"\n",
    "    \n",
    "    ### create some text case for input\n",
    "    case1 = '@VirginAmerica What @dhepburn said.'\n",
    "    case2 = \"@VirginAmerica plus you've added commercials to the experience... tacky.\"\n",
    "    case3 = \"@VirginAmerica I didn't today... Must mean I need to take another trip!\"\n",
    "    \n",
    "    \n",
    "    ## function output\n",
    "    case1_func = process_text(case1)\n",
    "    case2_func = process_text(case2)\n",
    "    case3_func = process_text(case3)\n",
    "    \n",
    "    ## expected output\n",
    "    case1_exp = ['@virginamerica', '@dhepburn', 'said']\n",
    "    case2_exp = ['@virginamerica', 'plu', 'ad', 'commerci', 'experi', '...', 'tacki']\n",
    "    case3_exp = ['@virginamerica', 'today', '...','must', 'mean','need','take','anoth','trip',\n",
    "'!']\n",
    "\n",
    "    \n",
    "    print(case1_func == case1_exp)\n",
    "    \n",
    "    print(case2_func == case2_exp)\n",
    "    print(case3_func == case3_exp)\n",
    "    \n",
    "    \n",
    "    assert case1_func == case1_exp\n",
    "    assert case2_func == case2_exp\n",
    "    assert case3_func == case3_exp\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "test_process_text_happy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process_text_unhappy_none_input():\n",
    "    \"\"\"\n",
    "    Test the functionality of process text input\n",
    "    The bad case is when the input is none or the input is not string, \n",
    "    which will raise errors.\n",
    "    \"\"\"\n",
    "    with pytest.raises(ValueError) as excinfo:\n",
    "        input_text = None\n",
    "        output = process_text(input_text)\n",
    "      \n",
    "    print(str(excinfo.value) == \"The input value is not valid.\")\n",
    "    assert str(excinfo.value) == \"The input value is not valid.\"\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_process_text_unhappy_nonstring_input():\n",
    "    \"\"\"\n",
    "    Test the functionality of process text input\n",
    "    The bad case is when the input is none or the input is not string, \n",
    "    which will raise errors.\n",
    "    \"\"\"\n",
    "    with pytest.raises(TypeError) as excinfo:\n",
    "        input_text = 123456\n",
    "        output = process_text(input)\n",
    "      \n",
    "    print(str(excinfo.value) == \"The input type is not string.\")\n",
    "    assert str(excinfo.value) == \"The input type is not string.\"\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-0445eff53eb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_process_text_unhappy_none_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-32-760575095797>\u001b[0m in \u001b[0;36mtest_process_text_unhappy_none_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mwill\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mpytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcinfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytest' is not defined"
     ]
    }
   ],
   "source": [
    "test_process_text_unhappy_none_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pytest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-a72c5b785861>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_process_text_unhappy_nonstring_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-33-97cd22771d19>\u001b[0m in \u001b[0;36mtest_process_text_unhappy_nonstring_input\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mwhich\u001b[0m \u001b[0mwill\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \"\"\"\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mpytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcinfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m123456\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytest' is not defined"
     ]
    }
   ],
   "source": [
    "test_process_text_unhappy_nonstring_input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-50cc50b2c685>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msmall_test\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "small_test =data.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9d9b79bd4d81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'small_test' is not defined"
     ]
    }
   ],
   "source": [
    "df_func = clean(small_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(clean(small_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'small_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-85208fafef80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'small_test' is not defined"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(np.array(clean(small_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-f32fa94d412f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m\"airline_sentiment\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "\"airline_sentiment\" not in data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    \"\"\" This functio will return the clean dataset including token feature columns\n",
    "    Args:\n",
    "        data (:py:class:`pandas.DataFrame`): DataFrame of the original dataset\n",
    "    \n",
    "    Return:\n",
    "        data (:py:class:`pandas.DataFrame`): DataFrame that is ready for feature generation\n",
    "    \"\"\"\n",
    "    ## check whether the required column exist or not before proceeding the cleaning\n",
    "    if \"airline_sentiment\" not in data.columns:\n",
    "        raise ValueError(\"The sentiment label does not exist.\")\n",
    "    \n",
    "    ## filter out neutral class\n",
    "    data=data[data['airline_sentiment']!='neutral']\n",
    "    data=data[data['airline_sentiment_confidence']==1.0]\n",
    "    ## define tokenizer\n",
    "    #Tokenizer = TweetTokenizer()\n",
    "    \n",
    "    ## call process_text helper function\n",
    "    logger.info(\"Ready to process raw text into tokens\")\n",
    "    data['tokens']=data['text'].apply(process_text)\n",
    "    logger.info(\"Successfully process raw text into tokens\")\n",
    "    ## convert the class as integer\n",
    "    data['positive']=(data['airline_sentiment']=='positive').astype(int)\n",
    "    ## define the final data for processing\n",
    "    data = data[[\"airline\",\"tokens\",\"airline_sentiment\",\"positive\"]]\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Virgin America',\n",
       "        list(['@virginamerica', 'realli', 'aggress', 'blast', 'obnoxi', 'entertain', 'guest', 'face', 'littl', 'recours']),\n",
       "        'negative', 0],\n",
       "       ['Virgin America',\n",
       "        list(['@virginamerica', 'realli', 'big', 'bad', 'thing']),\n",
       "        'negative', 0]], dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([['Virgin America',\n",
    "        list(['@virginamerica', 'realli', 'aggress', 'blast', 'obnoxi', 'entertain', 'guest', 'face', 'littl', 'recours']),\n",
    "        'negative', 0],\n",
    "       ['Virgin America',\n",
    "        list(['@virginamerica', 'realli', 'big', 'bad', 'thing']),\n",
    "        'negative', 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clean_happy():\n",
    "    \"\"\"\n",
    "    This is the function to test clean function, as a happy path\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(\"test_data/test_Tweets.csv\")\n",
    "    \n",
    "    small_test = data.iloc[:5]\n",
    "    \n",
    "    ## function output\n",
    "    df_func = clean(small_test)\n",
    "    \n",
    "    ## expected output\n",
    "    array = np.array([['Virgin America',\n",
    "        list(['@virginamerica', 'realli', 'aggress', 'blast', 'obnoxi', 'entertain', 'guest', 'face', 'littl', 'recours']),\n",
    "        'negative', 0],\n",
    "       ['Virgin America',\n",
    "        list(['@virginamerica', 'realli', 'big', 'bad', 'thing']),\n",
    "        'negative', 0]])\n",
    "    df_exp = pd.DataFrame(array, columns=[[\"airline\",\"tokens\",\"airline_sentiment\",\"positive\"]])\n",
    "    \n",
    "    print(df_func)\n",
    "    print(df_exp)\n",
    "    print(df_func.values == (df_exp.values))\n",
    "    \n",
    "    assert df_func.values.all() == df_exp.values.all()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          airline                                             tokens  \\\n",
      "3  Virgin America  [@virginamerica, realli, aggress, blast, obnox...   \n",
      "4  Virgin America          [@virginamerica, realli, big, bad, thing]   \n",
      "\n",
      "  airline_sentiment  positive  \n",
      "3          negative         0  \n",
      "4          negative         0  \n",
      "          airline                                             tokens  \\\n",
      "0  Virgin America  [@virginamerica, realli, aggress, blast, obnox...   \n",
      "1  Virgin America          [@virginamerica, realli, big, bad, thing]   \n",
      "\n",
      "  airline_sentiment positive  \n",
      "0          negative        0  \n",
      "1          negative        0  \n",
      "[[ True  True  True  True]\n",
      " [ True  True  True  True]]\n"
     ]
    }
   ],
   "source": [
    "test_clean_happy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.drop(\"airline_sentiment\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clean_unhappy():\n",
    "    \"\"\"\n",
    "    This function will be an unhappy path test\n",
    "    The unhappy case is when the \n",
    "    \"\"\"\n",
    "    \n",
    "    data = pd.read_csv(\"test_data/test_Tweets.csv\")\n",
    "    \n",
    "    small_test = data.iloc[:5]\n",
    "    ## make the data sentiment to be removed\n",
    "    small_test = small_test.drop(\"airline_sentiment\",axis = 1)\n",
    "    \n",
    "    print(small_test)\n",
    "    with pytest.raises(ValueError) as excinfo:\n",
    "        output = clean(small_test)\n",
    "      \n",
    "    print(str(excinfo.value) == \"The sentiment label does not exist.\")\n",
    "    assert str(excinfo.value) == \"The sentiment label does not exist.\"\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             tweet_id  airline_sentiment_confidence negativereason  \\\n",
      "0  570306133677760513                        1.0000            NaN   \n",
      "1  570301130888122368                        0.3486            NaN   \n",
      "2  570301083672813571                        0.6837            NaN   \n",
      "3  570301031407624196                        1.0000     Bad Flight   \n",
      "4  570300817074462722                        1.0000     Can't Tell   \n",
      "\n",
      "   negativereason_confidence         airline airline_sentiment_gold  \\\n",
      "0                        NaN  Virgin America                    NaN   \n",
      "1                     0.0000  Virgin America                    NaN   \n",
      "2                        NaN  Virgin America                    NaN   \n",
      "3                     0.7033  Virgin America                    NaN   \n",
      "4                     1.0000  Virgin America                    NaN   \n",
      "\n",
      "         name negativereason_gold  retweet_count  \\\n",
      "0     cairdin                 NaN              0   \n",
      "1    jnardino                 NaN              0   \n",
      "2  yvonnalynn                 NaN              0   \n",
      "3    jnardino                 NaN              0   \n",
      "4    jnardino                 NaN              0   \n",
      "\n",
      "                                                text tweet_coord  \\\n",
      "0                @VirginAmerica What @dhepburn said.         NaN   \n",
      "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
      "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
      "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
      "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
      "\n",
      "               tweet_created tweet_location               user_timezone  \n",
      "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
      "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
      "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
      "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
      "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pytest' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-243d1d1c545d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_clean_unhappy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-45-5472671a86ad>\u001b[0m in \u001b[0;36mtest_clean_unhappy\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mpytest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraises\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexcinfo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmall_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pytest' is not defined"
     ]
    }
   ],
   "source": [
    "test_clean_unhappy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "### featurize\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    \n",
    "def featurize(train,test, method, min_df=5, **kwargs):\n",
    "    \"\"\"This function allows user to specify either bag-of-words-counts\n",
    "    or tf-idf method for creating text features\n",
    "    Args:\n",
    "        train (:py:class:`pandas.DataFrame`): train DataFrame that is ready for feature generation \n",
    "        test (:py:class:`pandas.DataFrame`): test DataFrame that is ready for feature generation \n",
    "        method (str): specify the method for text feature generation including tfidf and bags of words\n",
    "        min_df (int): used for removing terms that appear too infrequently, default = 5, ignore terms that appear in less than 5 documents\".\n",
    "    \n",
    "    Return:\n",
    "        X_train (class 'numpy.float64') : train sparse matrix of text features importance indicator\n",
    "        X_test  (class 'numpy.float64') : test sparse matrix of text features importance indicator, having same number of columns as X_train\n",
    "        features_list (list): list of text features, length equal to the number of columns of X_train and X_test\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "\n",
    "    train = list(train[\"tokens\"].apply(lambda x: \" \".join(x)))\n",
    "    test = list(test[\"tokens\"].apply(lambda x: \" \".join(x)))\n",
    "    \n",
    "\n",
    "    if method == \"tf-idf\":\n",
    "        vectoriser = TfidfVectorizer(min_df = min_df, tokenizer = lambda s: s.split(' '))\n",
    "    elif method == \"bag-of-words\":\n",
    "        vectoriser = CountVectorizer(min_df = min_df, tokenizer = lambda s: s.split(' '))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown text feature methods.\")\n",
    "\n",
    "    \n",
    "    X_train = vectoriser.fit_transform(train)\n",
    "    X_test = vectoriser.transform(test)    \n",
    "    features_list = vectoriser.get_feature_names()\n",
    "    \n",
    "    if X_train.shape[1] != X_test.shape[1] and X_test.shape[1]!= len(features_list):\n",
    "        raise ValueError(\"input column dimension not correspond\")\n",
    "        \n",
    "    return X_train, X_test, features_list\n",
    "  \n",
    "    \n",
    "  \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = pd.read_csv(\"test_data/test_Tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_featurize_happy():\n",
    "    \"\"\"\n",
    "    This function will be a happy path for the unit test of generating text feature\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(\"test_data/test_Tweets.csv\")\n",
    "    small_train = data.iloc[:100]  \n",
    "    small_test = data.iloc[100:150]\n",
    "    ## function output\n",
    "    small_train = clean(small_train)\n",
    "    small_test = clean(small_test)\n",
    "    \n",
    "    train_func, test_func, features_func = featurize(small_train,small_test,\"tf-idf\", min_df=5)\n",
    "    \n",
    "    \n",
    "    ## expected output\n",
    "    train = list(small_train[\"tokens\"].apply(lambda x: \" \".join(x)))\n",
    "    test = list(small_test[\"tokens\"].apply(lambda x: \" \".join(x)))\n",
    "    vectoriser = TfidfVectorizer(min_df = 5, tokenizer = lambda s: s.split(' '))\n",
    "    \n",
    "    train_exp = vectoriser.fit_transform(train)\n",
    "    test_exp = vectoriser.transform(test)    \n",
    "    features_exp = vectoriser.get_feature_names() \n",
    "\n",
    "    \n",
    "    print(train_exp == train_func)\n",
    "    print(pd.DataFrame(train_func).all() == ((pd.DataFrame(train_exp)).all()))\n",
    "    \n",
    "    return train_exp, train_func\n",
    "  #  print(pd.DataFrame(test_func) ==((pd.DataFrame(test_exp))))\n",
    "    \n",
    "  #  print(features_exp == features_func)\n",
    "    \n",
    "   # assert train_exp.all() == train_func.all()\n",
    "    \n",
    "    #assert test_exp == test_func\n",
    "    assert pd.DataFrame(train_func).all().equals((pd.DataFrame(train_exp)).all())\n",
    "    \n",
    "   # assert pd.DataFrame(test_func).all().equals((pd.DataFrame(test_exp)).all())\n",
    "    \n",
    "  #  assert features_exp == features_func\n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\tTrue\n",
      "  (0, 1)\tTrue\n",
      "  (0, 2)\tTrue\n",
      "  (0, 3)\tTrue\n",
      "  (0, 4)\tTrue\n",
      "  (0, 5)\tTrue\n",
      "  (0, 6)\tTrue\n",
      "  (1, 0)\tTrue\n",
      "  (1, 1)\tTrue\n",
      "  (1, 2)\tTrue\n",
      "  (1, 3)\tTrue\n",
      "  (1, 4)\tTrue\n",
      "  (1, 5)\tTrue\n",
      "  (1, 6)\tTrue\n",
      "  (2, 0)\tTrue\n",
      "  (2, 1)\tTrue\n",
      "  (2, 2)\tTrue\n",
      "  (2, 3)\tTrue\n",
      "  (2, 4)\tTrue\n",
      "  (2, 5)\tTrue\n",
      "  (2, 6)\tTrue\n",
      "  (3, 0)\tTrue\n",
      "  (3, 1)\tTrue\n",
      "  (3, 2)\tTrue\n",
      "  (3, 3)\tTrue\n",
      "  :\t:\n",
      "  (45, 3)\tTrue\n",
      "  (45, 4)\tTrue\n",
      "  (45, 5)\tTrue\n",
      "  (45, 6)\tTrue\n",
      "  (46, 0)\tTrue\n",
      "  (46, 1)\tTrue\n",
      "  (46, 2)\tTrue\n",
      "  (46, 3)\tTrue\n",
      "  (46, 4)\tTrue\n",
      "  (46, 5)\tTrue\n",
      "  (46, 6)\tTrue\n",
      "  (47, 0)\tTrue\n",
      "  (47, 1)\tTrue\n",
      "  (47, 2)\tTrue\n",
      "  (47, 3)\tTrue\n",
      "  (47, 4)\tTrue\n",
      "  (47, 5)\tTrue\n",
      "  (47, 6)\tTrue\n",
      "  (48, 0)\tTrue\n",
      "  (48, 1)\tTrue\n",
      "  (48, 2)\tTrue\n",
      "  (48, 3)\tTrue\n",
      "  (48, 4)\tTrue\n",
      "  (48, 5)\tTrue\n",
      "  (48, 6)\tTrue\n",
      "Series([], dtype: bool)\n"
     ]
    }
   ],
   "source": [
    "a,b = test_featurize_happy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: bool)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(a).all() == pd.DataFrame(b).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<49x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 96 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(<49x7 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 96 stored elements in Compressed Sparse Row format>, dtype=object)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_featurize_unhappy():\n",
    "    \"\"\"\n",
    "    This function will be an unhappy path for the unit test of generating text feature\n",
    "    The bad case is that the specifying an unknown text processing method.\n",
    "    \"\"\"\n",
    "    \n",
    "    with pytest.raises(ValueError) as excinfo:\n",
    "        data = pd.read_csv(\"test_data/test_Tweets.csv\")\n",
    "        ## we deliberately give the unknown name of text processing method\n",
    "        small_train = data.iloc[:100]  \n",
    "        small_test = data.iloc[100:150]\n",
    "        \n",
    "        ## function output\n",
    "        small_train = clean(small_train)\n",
    "        small_test = clean(small_test)\n",
    "    \n",
    "        train_func, test_func, features_func = featurize(small_train,small_test,\"tfidf\", min_df=5)\n",
    "    \n",
    "    \n",
    "    print(str(excinfo.value) == \"Unknown text feature methods.\")\n",
    "    assert str(excinfo.value) == \"Unknown text feature methods.\"\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "test_featurize_unhappy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(__name__)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "import pickle\n",
    "#from src.featurize import featurize\n",
    "import yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data,trainSize = 0.7, randomState = 1, train_save_path=None,**kwargs):\n",
    "    \"\"\"This function split the train and test data\n",
    "    Args:\n",
    "        data (:py:class:`pandas.DataFrame`): the dataframe including features and labelthat are cleaned and tokenized\n",
    "        train_size (float): proportion of training size default 0.7\n",
    "        random_state (int): random_state for performance reproduction\n",
    "        train_save_path (str): train csv save path, default None\n",
    "    \n",
    "    Return:\n",
    "        train (:py:class:`pandas.DataFrame`): the dataframe of training data including features and labelthat are cleaned and tokenized\n",
    "        test (:py:class:`pandas.DataFrame`): the dataframe of training data including features and labelthat are cleaned and tokenized\n",
    "    \"\"\"\n",
    "    if data is None:\n",
    "        raise ValueError(\"No input data is ready to split.\")\n",
    "        \n",
    "    # Randomly split indexes\n",
    "    try:\n",
    "        index_train, index_test  = train_test_split(np.array(data.index), train_size=trainSize, \n",
    "                                                random_state = randomState, stratify=data['positive'])\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        logger.error(\"Fail to split the original data and check the original data dimensions\")\n",
    "    \n",
    "    # Write training and test sets \n",
    "    train = data.loc[index_train,:].copy()\n",
    "    test =  data.loc[index_test,:].copy()\n",
    "    # save the train csv\n",
    "    train.to_csv(train_save_path)\n",
    "    ##test data will be saved by user defined directory\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_test_data = pd.read_csv(\"test_data/test_Tweets_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = split_data(split_test_data,trainSize = 0.7, randomState = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_happy():\n",
    "    \"\"\"\n",
    "    Test functionality of splitting train test data\n",
    "    \"\"\"\n",
    "    \n",
    "    split_test_data = pd.read_csv(\"test_data/test_Tweets_clean.csv\")\n",
    "    function_train, function_test = split_data(split_test_data,trainSize = 0.7, randomState = 1)\n",
    "    \n",
    "    ## expected output\n",
    "    index_train, index_test  = train_test_split(np.array(split_test_data.index), train_size=0.7, \n",
    "                                                random_state = 1, stratify=split_test_data['positive'])\n",
    "    \n",
    "    expected_train = split_test_data.loc[index_train,:].copy()\n",
    "    expected_test = split_test_data.loc[index_test,:].copy()\n",
    "    \n",
    "    print(function_train.equals(expected_train))\n",
    "    print(function_test.equals(expected_test))\n",
    "    assert function_train.equals(expected_train)\n",
    "    assert function_test.equals(expected_test)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "split_data_happy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_unhappy():\n",
    "    \"\"\"\n",
    "    Test functionality of splitting train test data\n",
    "    The bad case is there is no input data for splitting\n",
    "    \"\"\"\n",
    "    with pytest.raises(ValueError) as excinfo:\n",
    "        split_test_data = None\n",
    "        function_train, function_test = split_data(split_test_data,trainSize = 0.7, randomState = 1)\n",
    "    \n",
    "    print(str(excinfo.value) == \"No input data is ready to split.\")\n",
    "    assert str(excinfo.value) == \"No input data is ready to split.\"\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "split_data_unhappy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1958"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model_logistic(data, transformed_feature , Cs = 50, fitIntercept=True, penalty=\"l2\", target_column=\"positive\", save_tmo =\"data/sentiment_class_prediction.pkl\" , **kwargs):\n",
    "    \"\"\"This function will train the logistic regression model by training data\n",
    "    Args: \n",
    "        data (:py:class:`pandas.DataFrame` or :py:class:`numpy.Array`): Training data\n",
    "        transformed_feature (class 'numpy.float64') : sparse matrix of text features importance indicator\n",
    "        target_column (str): column name of target\n",
    "        Cs: (int): the range of model complex parameter that will run for cross validation\n",
    "        fitIntercept (boolean) : boolean to indicate whether to fit intercept\n",
    "        penalty (str) : either \"l1\" or \"l2\"\n",
    "        save_tmo (str): Path to save the trained model.\n",
    "        **kwargs: Should contain arguments for specific requirements of model.\n",
    "        \n",
    "    Returns:\n",
    "        logit ('sklearn.linear_model.logistic.LogisticRegression'): Logistic regression model trained.\n",
    "    \"\"\"\n",
    "    ## fit logistic regression\n",
    "    ### check whether the transformed spare matrix number of rows is the same as the number of rows in the training data\n",
    "    \n",
    "    if data.shape[0] !=transformed_feature.shape[0]:\n",
    "        raise ValueError(\"Input training features sparse matrix does not match the row number of response.\")\n",
    "            \n",
    "    ### define X_train and y_train\n",
    "    \n",
    "  #  X_train = transformed_feature\n",
    "    y_train = data[target_column]\n",
    "    \n",
    "    ## perform cross validation\n",
    "    logit_l2= LogisticRegressionCV(Cs = Cs, fit_intercept = fitIntercept, penalty= penalty, solver='liblinear', scoring='neg_log_loss')\n",
    "    logit_l2.fit(transformed_feature, y_train)\n",
    "    \n",
    "    ## Refit the model with the best complex parameter \n",
    "    logit = LogisticRegression(C = logit_l2.C_[0], penalty=penalty, solver='liblinear')\n",
    "    logit.fit(transformed_feature, y_train)\n",
    "    \n",
    "    # Save the trained model object\n",
    "    if save_tmo is not None:\n",
    "        with open(save_tmo, \"wb\") as f:\n",
    "            pickle.dump(logit, f)\n",
    "        logger.info(\"Trained model object saved to %s\", save_tmo)\n",
    "\n",
    "    return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_test = pd.read_csv(\"test_data/test_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_train, index_test  = train_test_split(np.array(model_test.index), train_size=0.7, \n",
    "                                                random_state = 1, stratify=model_test['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "    train = model_test.loc[index_train,:].copy()\n",
    "    test =  model_test.loc[index_test,:].copy()\n",
    "    # save the train csv\n",
    "    train.to_csv(\"test_data/test_model_trainset.csv\")\n",
    "    test.to_csv(\"test_data/test_model_testset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CreditScore', 'Tenure', 'NumOfProducts', 'Gender', 'Geography']"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_kwargs[\"choose_features\"][\"features_to_use\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_model_logistic_happy():\n",
    "    \"\"\"\n",
    "    This functionality will test the logistic model training\n",
    "    A happy test assert the correct model type, correct coefficient value, correct input type\n",
    "    \"\"\"\n",
    "    \n",
    "    model_test_train = pd.read_csv(\"test_data/test_model_trainset.csv\")\n",
    "    \n",
    "    model_kwargs = {    \"features\" : [\"visible_mean_distribution\",\"visible_contrast\",\"visible_entropy\",\"visible_second_angular_momentum\"],\n",
    "        \"train_model_logistic\":{\n",
    "                          'target_column':'class',\n",
    "                     \"Cs\": 50,\n",
    "                    \"fitIntercept\": True,\n",
    "                    \"penalty\":\"l2\",\n",
    "                    \"save_tmo\": \"result_data/model_func_result.pkl\"}\n",
    "                   }\n",
    "    \n",
    "    X_train = model_test_train.loc[:,model_kwargs[\"features\"]]\n",
    "    function_output = train_model_logistic(model_test_train, X_train, **model_kwargs[\"train_model_logistic\"])\n",
    "    \n",
    "    ### expected\n",
    "    y_train = model_test_train[\"class\"]\n",
    "    logit_l2 = LogisticRegressionCV(Cs = 50, fit_intercept = True, penalty= \"l2\", solver='liblinear', scoring='neg_log_loss')\n",
    "    logit_l2.fit(X_train, y_train)\n",
    "    \n",
    "    ## Refit the model with the best complex parameter \n",
    "    expected_output = LogisticRegression(C = logit_l2.C_[0], penalty=\"l2\", solver='liblinear')\n",
    "    expected_output.fit(X_train, y_train)\n",
    "    \n",
    "    print(str(type(expected_output)))\n",
    "    print(str(type(function_output)))\n",
    "    \n",
    "    print(str(type(expected_output)) == str(type(function_output)))\n",
    "    ### check the assertion the model types are the same\n",
    "    assert str(type(expected_output)) == str(type(function_output))\n",
    "    \n",
    "    ### check the assert the model coefficients are the same\n",
    "    print(str(function_output.coef_) == str(expected_output.coef_))\n",
    "    assert str(function_output.coef_) == str(expected_output.coef_)\n",
    "    \n",
    "    ### check whether the model setting are the same\n",
    "    print(str(function_output) == str(expected_output))\n",
    "    assert str(function_output) == str(expected_output)\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_train_model_logistic_unhappy():\n",
    "    \"\"\"\n",
    "    This functionality will test the logistic model training\n",
    "    A unhappy test assert the incorrect input type\n",
    "    \"\"\"\n",
    "    with pytest.raises(ValueError) as excinfo:\n",
    "        model_test_train = pd.read_csv(\"test_data/test_model_trainset.csv\")\n",
    "\n",
    "        model_kwargs = {    \"features\" : [\"visible_mean_distribution\",\"visible_contrast\",\"visible_entropy\",\"visible_second_angular_momentum\"],\n",
    "            \"train_model_logistic\":{\n",
    "                              'target_column':'class',\n",
    "                         \"Cs\": 50,\n",
    "                        \"fitIntercept\": True,\n",
    "                        \"penalty\":\"l2\",\n",
    "                        \"save_tmo\": \"result_data/model_func_result.pkl\"}\n",
    "                       }\n",
    "\n",
    "        ## we deliberately select only a half rows of X_train\n",
    "        X_train = model_test_train.loc[:len(model_test_train)/2,model_kwargs[\"features\"]]\n",
    "        function_output = train_model_logistic(model_test_train, X_train, **model_kwargs[\"train_model_logistic\"])\n",
    "    print(str(excinfo.value) == \"Input training features sparse matrix does not match the row number of response.\")\n",
    "    assert str(excinfo.value) == \"Input training features sparse matrix does not match the row number of response.\"\n",
    "\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def score_model(transformed_feature,thres, path_to_tmo=None, **kwargs):\n",
    "    \"\"\"Get prediction results for the test set.\n",
    "    Args:\n",
    "        df (:py:class:`pandas.DataFrame`): Dataframe containing data to run prediction on.(test.csv data)\n",
    "        transformed_features (sparse matrix): sparse text matrix for test data \n",
    "        target_column (str): name of target column\n",
    "        path_to_tmo (str): Path to trained model.\n",
    "        save_path (str): Path to save prediction and summary results. \n",
    "    \n",
    "    Returns:\n",
    "        y_predicted (:py:class:`pandas.DataFrame`): DataFrame containing predicted scores.\n",
    "    \n",
    "    \"\"\"\n",
    "    # load the model saved from previous step\n",
    "    try:\n",
    "         with open(path_to_tmo, \"rb\") as f:\n",
    "                model = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        logger.error(\"Failure to load the model from the directory.\")\n",
    "    \n",
    "    ## prepare testing features and response\n",
    "   # X_test = df[chosen_features]\n",
    "  #  y_test = df[target_column]\n",
    "    \n",
    "    ## generate prediction with handling wrong/inconsistent input of chosen features\n",
    "    try:\n",
    "        ypred_proba_test = model.predict_proba(transformed_feature)[:,1]\n",
    "       #ypred_bin_test = model.predict(X_test)\n",
    "        ypred_bin_test = np.where(model.predict_proba(transformed_feature)[:,1] > thres,1,0)\n",
    "    \n",
    "    except:\n",
    "        raise ValueError(\"Test data features dimension does not match the model training feature dimension\")\n",
    "        logger.error(\"Test data features dimension does not match the model training feature dimension\")\n",
    "    \n",
    "\n",
    "    \n",
    "    ## output the prediction\n",
    "    df_prediction = pd.DataFrame([ypred_proba_test,ypred_bin_test],index = [\"predicted_proba\",\"predicted_class\"]).T\n",
    "   # df_prediction.to_csv(\"test_prediction.csv\")\n",
    "    \n",
    "    \n",
    "    return df_prediction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score_predict_happy():\n",
    "    \"\"\"\n",
    "    This functionality will test the score prediction function\n",
    "    A happy test assert the correct prediction result\n",
    "    \n",
    "    \"\"\"\n",
    "    ## function result\n",
    "    predict_testset = pd.read_csv(\"test_data/test_model_testset.csv\")\n",
    "    predict_train = pd.read_csv(\"test_data/test_model_trainset.csv\")\n",
    "    \n",
    "    model_kwargs = {\"features\" : [\"visible_mean_distribution\",\"visible_contrast\",\"visible_entropy\",\"visible_second_angular_momentum\"],\n",
    "        \"train_model_logistic\":{\n",
    "                          'target_column':'class',\n",
    "                     \"Cs\": 50,\n",
    "                    \"fitIntercept\": True,\n",
    "                    \"penalty\":\"l2\",\n",
    "                    \"save_tmo\": \"result_data/model_func_result.pkl\"},\n",
    "        \"score_model\": {\n",
    "        \"thres\": 0.5,\n",
    "        \"path_to_tmo\" : \"result_data/model_func_result.pkl\"} \n",
    "    }\n",
    "    X_train = predict_train.loc[:,model_kwargs[\"features\"]]\n",
    "    X_test = predict_testset.loc[:,model_kwargs[\"features\"]]\n",
    "    \n",
    "    function_output = score_model(X_test,**model_kwargs[\"score_model\"])\n",
    "    \n",
    "    function_prob = function_output[\"predicted_proba\"]\n",
    "    function_class = function_output[\"predicted_class\"]\n",
    "        \n",
    "    y_train = predict_train[\"class\"]\n",
    "    y_test = predict_testset[\"class\"]\n",
    "    logit_l2 = LogisticRegressionCV(Cs = 50, fit_intercept = True, penalty= \"l2\", solver='liblinear', scoring='neg_log_loss')\n",
    "    logit_l2.fit(X_train, y_train)\n",
    "    ## Refit the model with the best complex parameter \n",
    "    expected_output = LogisticRegression(C = logit_l2.C_[0], penalty=\"l2\", solver='liblinear')\n",
    "    expected_output.fit(X_train, y_train)\n",
    "    expected_prob = expected_output.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    print(function_prob == expected_prob)\n",
    "    print(sum(function_prob == expected_prob) == len(function_prob))\n",
    "    \n",
    "    print(function_prob.all() == expected_prob.all())\n",
    "    \n",
    " #  assert the function probability will between 0 and 1\n",
    "    assert function_prob.between(0,1,inclusive=True).all()\n",
    "    # assert the function class is either 0 or 1\n",
    "    assert function_class.isin([0,1]).all()\n",
    "    # assert the return probability of function is the same as the probability of expected output\n",
    "    assert function_prob.all() == expected_prob.all()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_score_predict_unhappy():\n",
    "    \"\"\"\n",
    "    This functionality will test the score prediction function\n",
    "    A unhappy test assert the the test data has incorrect dimensions with the model\n",
    "    \n",
    "    \"\"\"\n",
    "    ## function result\n",
    "    \n",
    "    with pytest.raises(ValueError) as excinfo:\n",
    "        \n",
    "        \n",
    "        predict_testset = pd.read_csv(\"test_data/test_model_testset.csv\")\n",
    "        predict_train = pd.read_csv(\"test_data/test_model_trainset.csv\")\n",
    "    \n",
    "        model_kwargs = {\"features\" : [\"visible_mean_distribution\",\"visible_entropy\",\"visible_second_angular_momentum\"],\n",
    "        \"train_model_logistic\":{\n",
    "                          'target_column':'class',\n",
    "                     \"Cs\": 50,\n",
    "                    \"fitIntercept\": True,\n",
    "                    \"penalty\":\"l2\",\n",
    "                    \"save_tmo\": \"result_data/model_func_result.pkl\"},\n",
    "        \"score_model\": {\n",
    "        \"thres\": 0.5,\n",
    "        \"path_to_tmo\" : \"result_data/model_func_result.pkl\"} \n",
    "        }\n",
    "        \n",
    "       # X_train = predict_train.loc[:,model_kwargs[\"features\"]]\n",
    "        X_test = predict_testset.loc[:,model_kwargs[\"features\"]]\n",
    "    \n",
    "        function_output = score_model(X_test,**model_kwargs[\"score_model\"])\n",
    "    \n",
    "        function_prob = function_output[\"predicted_proba\"]\n",
    "        function_class = function_output[\"predicted_class\"]\n",
    "     \n",
    "    print(str(excinfo.value) == \"Test data features dimension does not match the model training feature dimension\")\n",
    "        \n",
    "    assert str(excinfo.value) == \"Test data features dimension does not match the model training feature dimension\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "test_score_predict_unhappy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(df, y_predicted, target_column, **kwargs):\n",
    "    \"\"\"Evaluate the performance of the model   \n",
    "    Args:\n",
    "        df (:py:class:`pandas.DataFrame`): test Dataframe which containing true y label\n",
    "        target_column (str): column name of target column\n",
    "        y_predicted (:py:class:`pandas.DataFrame`): Dataframe containing predicted probability and score\n",
    "    \n",
    "    Returns: \n",
    "        confusion_df (:py:class:`pandas.DataFrame`): Dataframe reporting confusion matrix\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # get predicted scores\n",
    "        y_pred_prob = y_predicted.loc[:,\"predicted_proba\"]\n",
    "        y_pred = y_predicted.loc[:,\"predicted_class\"]\n",
    "        # get true labels\n",
    "        y_test = df.loc[:,target_column]\n",
    "         \n",
    "    # raise IndexError when the input dataframe does not have two columns as desired\n",
    "    except:\n",
    "        logger.error(\"Fail to prepare data for evaluation metrics\")\n",
    "        \n",
    "        \n",
    "    if len(y_pred)!=len(y_test):\n",
    "        raise IndexError('Index out of bounds!')   \n",
    "        # calculate auc and accuracy and f1_score if specified\n",
    "    if \"auc\" in kwargs[\"metrics\"]:\n",
    "        auc = sklearn.metrics.roc_auc_score(y_test, y_pred_prob)\n",
    "        print('AUC on test: %0.3f' % auc)\n",
    "    if \"accuracy\" in kwargs[\"metrics\"]:\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, y_pred)\n",
    "        print('Accuracy on test: %0.3f' % accuracy)\n",
    "    if \"f1_score\" in kwargs[\"metrics\"]:\n",
    "        f1 = sklearn.metrics.f1_score(y_test, y_pred)\n",
    "        print('F1-score on test: %0.3f' % f1)\n",
    "    \n",
    "#     ## evaluate the performance\n",
    "#     auc = sklearn.metrics.roc_auc_score(y_test, ypred_proba_test)\n",
    "    confusion = sklearn.metrics.confusion_matrix(y_test, y_pred)\n",
    "#     accuracy = sklearn.metrics.accuracy_score(y_test, ypred_bin_test)\n",
    "#     classification_report = sklearn.metrics.classification_report(y_test, ypred_bin_test)\n",
    "    \n",
    "    print(\"-------------- Model Performance Evaluation-------------------\")\n",
    "    print('AUC on test: %0.3f' % auc)\n",
    "    print('Accuracy on test: %0.3f' % accuracy)\n",
    "    print(pd.DataFrame(confusion,\n",
    "                  index=['Actual negative','Actual positive'],\n",
    "                  columns=['Predicted negative', 'Predicted positive']))\n",
    "    \n",
    "    ## save confusion matrix\n",
    "    confusion_df = pd.DataFrame(confusion,\n",
    "        index=['Actual Negative','Actual Positive'],\n",
    "        columns=['Predicted Negative', 'Predicted Positive'])\n",
    "    \n",
    "    ## save metrics performance as csv\n",
    "    metrics = pd.DataFrame([auc,accuracy,f1],index =[\"auc\",\"accuacy\",\"f1\"],columns=[\"Evaluation Metric\"])\n",
    "  #  metrics.to_csv(\"test_metrics.csv\")\n",
    "    \n",
    "    ## post-processing, feature importance, odd ratios\n",
    " #   fitted.to_csv(save_path + \"/model_inference.csv\")\n",
    "    return confusion_df, metrics\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_model_happy():\n",
    "    \"\"\"Test the functionality of evaluate_model.\"\"\"\n",
    "    # test data input\n",
    "    score_input = {\"predicted_proba\": [0.98,0,0.99,0.94,0.93,0,0.06,0.699,0.04,0.97],\n",
    "                   \"predicted_class\": [1,0,1,1,1,0,0,1,0,1]}\n",
    "    label_input = {'class':[0,1,0,1,0,1,0,0,1,0]}\n",
    "\n",
    "    score_df = pd.DataFrame(score_input)\n",
    "    label_df = pd.DataFrame(label_input)\n",
    "\n",
    "    # desired output dataframe\n",
    "    output = sklearn.metrics.confusion_matrix(label_df, score_df.iloc[:,1])\n",
    "    output_df = pd.DataFrame(output,\n",
    "        index=['Actual Negative','Actual Positive'],\n",
    "        columns=['Predicted Negative', 'Predicted Positive'])\n",
    "    \n",
    "    # add kwargs for function\n",
    "    pre_defined_kwargs = {\"target_column\": \"class\",\n",
    "                          'metrics':[\"auc\",\"accuracy\",\"f1_score\"]}\n",
    "    # raise AssertionError if dataframes do not match\n",
    "    \n",
    "    ## assert tjhe confusion matrix is the same\n",
    "    assert output_df.equals(evaluate_model(label_df, score_df, **pre_defined_kwargs)[0])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC on test: 0.125\n",
      "Accuracy on test: 0.200\n",
      "F1-score on test: 0.200\n",
      "-------------- Model Performance Evaluation-------------------\n",
      "AUC on test: 0.125\n",
      "Accuracy on test: 0.200\n",
      "                 Predicted negative  Predicted positive\n",
      "Actual negative                   1                   5\n",
      "Actual positive                   3                   1\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_model_happy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_evaluate_model_unhappy():\n",
    "    \"\"\"Test the functionality of evaluate_model.\"\"\"\n",
    "    # test data input\n",
    "    with pytest.raises(IndexError) as excinfo:\n",
    "        score_input = {\"predicted_proba\": [0.98,0,0.99,0.94,0.93,0,0.06,0.699],\n",
    "                       \"predicted_class\": [1,0,1,1,1,0,0,1]}\n",
    "        label_input = {'class':[0,1,0,1,0,1,0,0,1,0]}\n",
    "\n",
    "        score_df = pd.DataFrame(score_input)\n",
    "        label_df = pd.DataFrame(label_input)\n",
    "\n",
    "        # desired output dataframe\n",
    "      #  output = sklearn.metrics.confusion_matrix(label_df, score_df.iloc[:,1])\n",
    "     #   output_df = pd.DataFrame(output,\n",
    "       #     index=['Actual Negative','Actual Positive'],\n",
    "      #      columns=['Predicted Negative', 'Predicted Positive'])\n",
    "\n",
    "        # add kwargs for function\n",
    "        pre_defined_kwargs = {\"target_column\": \"class\",\n",
    "                              'metrics':[\"auc\",\"accuracy\",\"f1_score\"]}\n",
    "        # raise AssertionError if dataframes do not match\n",
    "        evaluate_model(label_df, score_df, **pre_defined_kwargs)[0]\n",
    "        ## assert tjhe confusion matrix is the same\n",
    "    print(str(excinfo.value) == 'Index out of bounds!')    \n",
    "    assert str(excinfo.value) == 'Index out of bounds!'\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "test_evaluate_model_unhappy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_coefficients(model, labels):\n",
    "    coef = model.coef_\n",
    "    table = pd.Series(coef.ravel(), index = labels).sort_values(ascending=True, inplace=False)\n",
    "    \n",
    "    all_ = True\n",
    "    if len(table) > 20:\n",
    "        reference = pd.Series(np.abs(coef.ravel()), index = labels).sort_values(ascending=False, inplace=False)\n",
    "        reference = reference.iloc[:20]\n",
    "        table = table[reference.index]\n",
    "        table = table.sort_values(ascending=True, inplace=False)\n",
    "        all_ = False\n",
    "        \n",
    "\n",
    "    fig, ax = fig, ax = plt.subplots()\n",
    "    table.T.plot(kind='barh', edgecolor='black', width=0.7, linewidth=.8, alpha=0.9, ax=ax)\n",
    "    ax.tick_params(axis=u'y', length=0) \n",
    "    if all_:\n",
    "        ax.set_title('Estimated coefficients', fontsize=14)\n",
    "    else: \n",
    "        ax.set_title('Estimated coefficients (twenty largest in absolute value)', fontsize=14)\n",
    "    sns.despine()\n",
    "\n",
    "    return fig, ax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inference(features, path_to_tmo, path_save_plot, **kwargs):\n",
    "    \"\"\"Get model inference and feature importance\n",
    "    Args:\n",
    "        features (list): list of text characters used as features output of the featurize\n",
    "        path_to_tmo (str): Path to trained model.\n",
    "        save_path (str): Path to save prediction and summary results. \n",
    "    \n",
    "    Returns:\n",
    "        fitted (:py:class:`pandas.DataFrame`): DataFrame containing odd ratio, coefficient value for each feature, descending ordered \n",
    "    \n",
    "    \"\"\"\n",
    "    # load the model saved from previous step\n",
    "    try:\n",
    "        with open(path_to_tmo, \"rb\") as f:\n",
    "                model = pickle.load(f)\n",
    "    except Exception as e:\n",
    "        logger.error(e)\n",
    "        logger.error(\"Failure to load the model from the directory.\")\n",
    "    \n",
    "    fitted = pd.DataFrame(index=features) \n",
    "    \n",
    "    if len(features)!= len(model.coef_[0]):\n",
    "        raise ValueError(\"Input feature name list does not correspond to the model coefficients\")\n",
    "    \n",
    "    fitted['coefs'] = model.coef_[0]\n",
    "    fitted['odds_ratio'] = fitted.coefs.apply(np.exp)\n",
    "    ## sort odd ration descending \n",
    "    fitted = fitted.sort_values(by='odds_ratio', ascending=False)\n",
    "    ## show the feature importance plot\n",
    "    fig,ax = plot_coefficients(model,features)\n",
    "    fig.savefig(path_save_plot)\n",
    "    \n",
    "    return fitted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_inference_happy():\n",
    "    \"\"\"\n",
    "    This is the test the functionality of model inference.\n",
    "    \"\"\"\n",
    "    \n",
    "    model_kwargs = {\"features\" : [\"visible_mean_distribution\",\"visible_contrast\",\"visible_entropy\",\"visible_second_angular_momentum\"],\n",
    "        \"train_model_logistic\":{\n",
    "                          'target_column':'class',\n",
    "                     \"Cs\": 50,\n",
    "                    \"fitIntercept\": True,\n",
    "                    \"penalty\":\"l2\",\n",
    "                    \"save_tmo\": \"result_data/model_func_result.pkl\"},\n",
    "        \"score_model\": {\n",
    "        \"thres\": 0.5,\n",
    "        \"path_to_tmo\" : \"result_data/model_func_result.pkl\"},\n",
    "         \"model_inference\": {\"features\" : [\"visible_mean_distribution\",\"visible_contrast\",\"visible_entropy\",\"visible_second_angular_momentum\"],\n",
    "        \"path_to_tmo\": \"result_data/model_func_result.pkl\",\n",
    "        \"path_save_plot\": \"result_data/test_plot.png\"\n",
    "         }           \n",
    "    }\n",
    "    ## function result\n",
    "    result = model_inference(**model_kwargs[\"model_inference\"])\n",
    "    \n",
    "    ## expected result\n",
    "    predict_testset = pd.read_csv(\"test_data/test_model_testset.csv\")\n",
    "    predict_train = pd.read_csv(\"test_data/test_model_trainset.csv\")\n",
    "\n",
    "    X_train = predict_train.loc[:,model_kwargs[\"features\"]]\n",
    "    X_test = predict_testset.loc[:,model_kwargs[\"features\"]]\n",
    "\n",
    "    function_output = train_model_logistic(predict_train, X_train, **model_kwargs[\"train_model_logistic\"])\n",
    "    \n",
    "    fitted_coef = pd.DataFrame(index=model_kwargs[\"features\"]) \n",
    "    fitted_coef['coefs'] = function_output.coef_[0]\n",
    "    fitted_coef['odds_ratio'] = fitted_coef.coefs.apply(np.exp)\n",
    "    fitted_coef = fitted_coef.sort_values(by='odds_ratio', ascending=False)\n",
    "    \n",
    "    print(result.equals(fitted_coef))\n",
    "    \n",
    "    assert result.equals(fitted_coef)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = test_model_inference_happy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_model_inference_unhappy():\n",
    "    \"\"\"\n",
    "    This is the test for the unhappy path of model inference function.\n",
    "    It will test the bad input when the input features list size do not match the \n",
    "    model coefficients list size\n",
    "    \n",
    "    \"\"\"\n",
    "    with pytest.raises(ValueError) as excinfo:\n",
    "        model_kwargs = {\n",
    "             \"model_inference\": {\"features\" : [\"visible_mean_distribution\",\"visible_contrast\", \"visible_second_angular_momentum\"],\n",
    "            \"path_to_tmo\": \"result_data/model_func_result.pkl\",\n",
    "            \"path_save_plot\": \"result_data/test_plot.png\"\n",
    "             }           \n",
    "        }\n",
    "        ## function result\n",
    "        result = model_inference(**model_kwargs[\"model_inference\"])\n",
    "    \n",
    "    print(str(excinfo.value) == \"Input feature name list does not correspond to the model coefficients\")    \n",
    "    assert str(excinfo.value) == \"Input feature name list does not correspond to the model coefficients\"\n",
    "    \n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "test_model_inference_unhappy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### test featurize function happy path\n",
    "\n",
    "def test_featurize_happy():\n",
    "    \"\"\"\n",
    "    Test the functionality of featurizing the text data\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
